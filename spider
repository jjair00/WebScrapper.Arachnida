#!/goinfre/jcueto-r/miniconda3/envs/42AI-jcueto-r/bin/python3
from bs4 import BeautifulSoup
from urllib.parse import * 
import urllib
import requests
import argparse
import os
from pathlib import Path,PurePath


def init():

    parser = argparse.ArgumentParser(
        prog="./spider",
        description="Home-made tool to scrap a website or a path local.",
        epilog="Arachnida's exercise of the Cybersecurity Bootcamp of Fundación 42 (Malaga)."
    )
    parser.add_argument("URL", help="URL del sitio web a 'scrapear'", type=str)
    parser.add_argument("-r", help="Indica que la búsqueda y descarga de imágenes será recursiva (por defecto L=5).", action="store_true")
    parser.add_argument("-l", choices= range(1, 6), help="Nivel de profundidad para la búsqueda y descarga de imágenes", type=int, default=None)
    parser.add_argument("-p", help="Ruta de la carpeta donde descargar las imágenes", type=str, default="./data/")

    args = parser.parse_args()

    if (args.r == False and args.l != None):
        print('ERROR')
    if (args.l == None):
        args.l = 5

    return parser.parse_args()



web_pages_found_filter = []
web_pages_found_filter1 = []
web_pages_found_filter2 = []
web_pages_found_filter3 = []
web_pages_found_filter4 = []
web_pages_found_filter5 = []


images_found = []

def url_detect(website): 
    try: 
        if website.startswith("file://"):
            web_pages_found_filter.append(website)

        elif website.startswith('http'):
            request_status_link1 = requests.get(website)
            parsed = urlparse(website)
            web_pages_found_filter.append(website)
            if request_status_link1.status_code == 200:
                html = BeautifulSoup(request_status_link1.content, 'lxml')
                id = html.find_all('a')
                for link1 in id:
                    href = link1.get('href')
                    parsed1 = urlparse(href) 
                    if parsed.netloc == parsed1.netloc and href not in web_pages_found_filter1:
                        web_pages_found_filter1.append(href)
            
            for link2 in web_pages_found_filter1:
                if link2.startswith('http'):
                    request_status_link2 = requests.get(link2)
                    if request_status_link2.status_code == 200:
                        html2 = BeautifulSoup(request_status_link2.content, 'lxml')
                        id2 = html2.find_all('a')
                        for link2_2 in id2:
                            href2 = link2_2.get('href')
                            parsed2 = urlparse(href2)
                            if parsed2.netloc == parsed.netloc and href2 not in web_pages_found_filter2:
                                web_pages_found_filter2.append(href2)

            for link3 in web_pages_found_filter2:
                if link3.startswith('http'):
                    request_status_link3 = requests.get(link3)
                    if request_status_link3.status_code == 200:
                        html3 = BeautifulSoup(request_status_link3.content, 'lxml')
                        id3 = html3.find_all('a')
                        for link3_3 in id3:
                            href3 = link3_3.get('href')
                            parsed3 = urlparse(href3)
                            if parsed3.netloc == parsed.netloc and href3 not in web_pages_found_filter3:
                                web_pages_found_filter3.append(href3)

            for link4 in web_pages_found_filter3:
                if link4.startswith('http'):
                    request_status_link4 = requests.get(link4)
                    if request_status_link4.status_code == 200:
                        html4 = BeautifulSoup(request_status_link4.content, 'lxml')
                        id4 = html4.find_all('a')
                        for link4_4 in id4:
                            href4 = link4_4.get('href')
                            parsed4 = urlparse(href4)
                            if parsed4.netloc == parsed.netloc and href4 not in web_pages_found_filter4:
                                web_pages_found_filter4.append(href4)

            for link5 in web_pages_found_filter4:
                if link5.startswith('http'):
                    request_status_link5 = requests.get(link5)
                    if request_status_link5.status_code == 200:
                        html5 = BeautifulSoup(request_status_link5.content, 'lxml')
                        id5 = html5.find_all('a')
                        for link5_5 in id5:
                            href5 = link5_5.get('href')
                            parsed5 = urlparse(href5)
                            if parsed5.netloc == parsed.netloc and href5 not in web_pages_found_filter5:
                                web_pages_found_filter5.append(href5)
    except requests.ConnectionError as e:
        print("Invalid URL. You should enter a correct URL.")
        
def search_images_http():   
    for x in web_pages_found_filter:
        request_status = requests.get(x)
        html_img = BeautifulSoup(request_status.content, 'lxml')
        imgs_filter = html_img.find_all('img')
        for images in imgs_filter:
            imgs = images.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_http1():
    for x1 in web_pages_found_filter1:
        request_status1 = requests.get(x1)
        html_img1 = BeautifulSoup(request_status1.content, 'lxml')
        imgs_filter_1 = html_img1.find_all('img')
        for images1 in imgs_filter_1:
            imgs = images1.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_http2():
    for x2 in web_pages_found_filter2:
        request_status2 = requests.get(x2)
        html_img2 = BeautifulSoup(request_status2.content, 'lxml')
        imgs_filter_2 = html_img2.find_all('img')
        for images2 in imgs_filter_2:
            imgs = images2.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_http3():
    for x3 in web_pages_found_filter3:
        request_status3 = requests.get(x3)
        html_img3 = BeautifulSoup(request_status3.content, 'lxml')
        imgs_filter_3 = html_img3.find_all('img')
        for images3 in imgs_filter_3:
            imgs = images3.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_http4():
    for x4 in web_pages_found_filter4:
        request_status4 = requests.get(x4)
        html_img4 = BeautifulSoup(request_status4.content, 'lxml')
        imgs_filter_4 = html_img4.find_all('img')
        for images4 in imgs_filter_4:
            imgs = images4.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_http5():
    for x5 in web_pages_found_filter5:
        request_status5 = requests.get(x5)
        html_img5 = BeautifulSoup(request_status5.content, 'lxml')
        imgs_filter_5 = html_img5.find_all('img')
        for images5 in imgs_filter_5:
            imgs = images5.get('src')
            if check_extension(imgs):
                images_found.append(imgs)
    return images_found

def search_images_file():
        parsed = urlparse(website)
        website1 = parsed.path
        with open(website1, 'r') as my_file:
            soup = BeautifulSoup(my_file, 'lxml')
        imgs_filter = soup.find_all('img')
        for images in imgs_filter: 
            imgs = images.get('src') 
            if check_extension(imgs):
                images_found.append(imgs)

        return images_found


def check_extension(imgs):
    extensions = ["jpg", "jpeg", "png", "gif", "bmp", "docx", "pdf"]

    for list in extensions:
        if imgs.endswith(list):
            return True


def get_img_path(images_found,folder):
    os.makedirs(folder, exist_ok=True)
    if folder == './data/':
        a = os.getcwd()
        b = a + '/data'
        folder = b
        os.chdir(folder)
    else:
        os.chdir(folder)
    try:
        for link5 in images_found:
            if link5.startswith("file://"):
                imagen_name = link5.split("/")[-1]
                with open(folder + "/" + imagen_name,'wb') as f:
                    urllib.request.urlretrieve(link5, imagen_name)
            if link5.startswith("http"):
                request_status_link5 = requests.get(link5, stream=True)
                imagen_name = link5.split("/")[-1]
                path = os.path.join(folder, imagen_name)
                with open(path, "bw" ) as img:
                    img.write(request_status_link5.content)
    except urllib.error.URLError as e:
        os.remove(imagen_name)
    
        


if __name__ == "__main__":

    args = init()

    website = args.URL
    level = args.l
    folder = args.p
    rec = args.r
    
    if website:
        print()
        print("Checking site....")
        print()
        if website.startswith("file://"):
            url_detect(website)
            search_images_file()
            get_img_path(images_found,folder)

        if website.startswith("http"):
            url_detect(website)
            if folder:
                if not rec:
                    search_images_http()
                elif rec:
                    if level == 1:
                        search_images_http1()
                    elif level == 2:
                        search_images_http2()
                    elif level == 3:
                        search_images_http3()
                    elif level == 4:
                        search_images_http4()
                    else:
                        search_images_http5()
            get_img_path(images_found, folder)
            print()

        